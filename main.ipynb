{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetching crypto and tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from TweetScraper import TweetScraper\n",
    "from CryptoApi import CryptoApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "# scrp = TweetScraper(start='2023-05-04', end='2023-05-14', maxEmptyPages=2, max_workers=8)\n",
    "# new_tweets = scrp.parallel_download_tweets()\n",
    "# new_twt = pd.DataFrame(new_tweets)\n",
    "\n",
    "\n",
    "# with open('crypto_token.txt','r') as f:\n",
    "#     token = f.readline()\n",
    "    \n",
    "# crypto = CryptoApi(token)\n",
    "# crypto.get_data('btc','usd','day',period_count=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tweets_df = TweetScraper().fetch_data('2023-04-10','2023-06-09', frequency='d')\n",
    "# cleaned_df = TweetCleaner().transform(new_tweets_df)\n",
    "# nlp_model = LDA().fit(cleaned_df)\n",
    "# cleaned_df = extract_topics(nlp_model.topics)\n",
    "\n",
    "# new_btc_df = CryptoApi().fetch_data()\n",
    "# cleaned_btc = CryptoCleaner().transform(new_btc_df)\n",
    "# xgb_model = XGBoost().fit(cleaned_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreparation.CryptoPreprocessor import CryptoPreprocessor\n",
    "from DataPreparation.TweetPreprocessor import TweetPreprocessor\n",
    "from DataPreparation.TextVectorizer import TextVectorizer\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "\n",
    "tweets_df = pd.read_csv(r'Data/elon_tweets.csv', index_col=0)\n",
    "twt_prep = TweetPreprocessor(tweets_df)\n",
    "mod_tweets_df = twt_prep.transform()\n",
    "\n",
    "text2vec = TextVectorizer()\n",
    "preprocessing_pipeline = text2vec.make_pipeline()\n",
    "id2word, corpus = preprocessing_pipeline.transform(mod_tweets_df['rawContent'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "\n",
    "def black_box_function(x, y):\n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "\n",
    "    This is just serving as an example, for all intents and\n",
    "    purposes think of the internals of this function, i.e.: the process\n",
    "    which generates its output values, as unknown.\n",
    "    \"\"\"\n",
    "    return -x ** 2 - (y - 1) ** 2 + 1\n",
    "\n",
    "def optimize_bayes(func, pbounds=None, verbose=2):\n",
    "    if pbounds==None:\n",
    "        pbounds = {'x': (2, 12), 'y': (-15, 15)}\n",
    "        \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=func,\n",
    "        pbounds=pbounds,\n",
    "        verbose=verbose, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "        random_state=1,\n",
    "    )\n",
    "    # optimizer.maximize(init_points=2,n_iter=3)\n",
    "            \n",
    "    # for i, res in enumerate(optimizer.res):\n",
    "    #     print(f\"Iteration {i}:\\n{res}\")\n",
    "                \n",
    "    # optimizer.set_bounds(new_bounds={\"x\": (-2, 3)})\n",
    "    optimizer.maximize(init_points=0, n_iter=15)\n",
    "            \n",
    "    # optimizer.probe(params={\"x\": 0.5, \"y\": 0.7}, lazy=True)\n",
    "    \n",
    "    logger = JSONLogger(path=\"./logs.log\")\n",
    "    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "    optimizer.maximize(init_points=2,n_iter=3)\n",
    "\n",
    "    \n",
    "# optimize_bayes(func=black_box_function, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_optimizer = BayesianOptimization(f=black_box_function,\n",
    "                                     pbounds={\"x\": (-2, 2), \"y\": (-2, 2)},\n",
    "                                     verbose=2,\n",
    "                                     random_state=7)\n",
    "print(len(new_optimizer.space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_logs(new_optimizer, logs=[\"./logs.log.json\"])\n",
    "print(\"New optimizer is now aware of {} points.\".format(len(new_optimizer.space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_optimizer.maximize(\n",
    "#     init_points=0,\n",
    "#     n_iter=10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(r\"D:\\Projects\\ElonMuskCrypto\\Models\\NLPmodels\\lda\")\n",
    "lda_model = LdaMulticore.load(temp_file)\n",
    "\n",
    "btc_df = pd.read_csv('Data/btc_data.csv', index_col=0)\n",
    "crypto_prep = CryptoPreprocessor()\n",
    "topics_btc = crypto_prep.transform(lda_model, mod_tweets_df, btc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,10))\n",
    "# sns.boxplot(data=topics_btc, x='month',y='open')\n",
    "# ax.set_title('open by month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "X = topics_btc.copy()\n",
    "X[\"tomorrow\"] = X[\"close\"].shift(-1)\n",
    "X[\"target\"] = (X[\"tomorrow\"] > X[\"close\"]).astype(int)\n",
    "\n",
    "target = 'target'\n",
    "predictors = [col for col in X if col not in ['target', 'tomorrow','close']]\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits=6)\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, min_samples_split=25, random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizons = [2,5,10,15,20,25]\n",
    "horizons = [2,5,60,250]\n",
    "\n",
    "def add_trend_season(data, horizons):\n",
    "    predictors = []\n",
    "    for horizon in horizons:\n",
    "        rolling_average = data.rolling(horizon).mean()\n",
    "        \n",
    "        ratio_column = f'close_ratio{horizon}'\n",
    "        data[ratio_column] = data['close'] / rolling_average['close']\n",
    "        \n",
    "        trend_column = f'Trend_{horizon}'\n",
    "        data[trend_column] = data.shift(1).rolling(horizon).sum()['target']\n",
    "        \n",
    "        predictors += [ratio_column, trend_column]\n",
    "        \n",
    "    return predictors\n",
    "\n",
    "\n",
    "new_predictors = add_trend_season(X, horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train, test, predictors, model):\n",
    "    model.fit(train[predictors], train['target'])\n",
    "    preds = model.predict_proba(test[predictors])[:,1]\n",
    "    preds[preds >= 0.65] = 1\n",
    "    preds[preds < 0.65]  = 0\n",
    "    preds = pd.Series(preds, index=test.index, name='Predictions')    \n",
    "    combined = pd.concat([test['target'], preds], axis=1)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def backtest(X, predictors, model):\n",
    "    all_preds = []\n",
    "    for (train_idx, test_idx) in tss.split(X):\n",
    "        train = X.iloc[train_idx]\n",
    "        test  = X.iloc[test_idx]        \n",
    "        \n",
    "        preds = predict(train, test, predictors, model)\n",
    "        all_preds.append(preds)\n",
    "        \n",
    "    return pd.concat(all_preds)\n",
    "\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits=2)\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, \n",
    "                                        min_samples_split=2, \n",
    "                                        random_state=1,\n",
    "                                        n_jobs=-1)\n",
    "\n",
    "# preds = backtest(X, predictors, baseline_model)    \n",
    "preds = backtest(X.dropna(), predictors, baseline_model)    \n",
    "print(preds['Predictions'].value_counts().values)\n",
    "precision_score(preds['target'], preds['Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_btc[\"tomorrow\"] = topics_btc[\"close\"].shift(-1)\n",
    "topics_btc[\"target\"] = (topics_btc[\"tomorrow\"] > topics_btc[\"close\"]).astype(int)\n",
    "horizons = [2,5,60,250]\n",
    "\n",
    "def add_trend_season(data, horizons, ignore_trend=False, ignore_season=False):\n",
    "    predictors = []\n",
    "    for horizon in horizons:\n",
    "        rolling_average = data.rolling(horizon).mean()\n",
    "        \n",
    "        if not ignore_trend:\n",
    "            ratio_column = f'close_ratio{horizon}'\n",
    "            data[ratio_column] = data['close'] / rolling_average['close']\n",
    "            predictors += [ratio_column]\n",
    "            \n",
    "        if not ignore_season:            \n",
    "            trend_column = f'Trend_{horizon}'\n",
    "            data[trend_column] = data.shift(1).rolling(horizon).sum()['target']\n",
    "            predictors += [trend_column]        \n",
    "        \n",
    "    return predictors\n",
    "\n",
    "\n",
    "# new_predictors = add_trend_season(topics_btc, horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_btc[\"tomorrow\"] = topics_btc[\"close\"].shift(-1)\n",
    "topics_btc[\"target\"] = (topics_btc[\"tomorrow\"] > topics_btc[\"close\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=150,                     \n",
    "                    max_leaves=50,\n",
    "                    learning_rate=0.1,\n",
    "                    random_state=1,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "data = topics_btc.copy()\n",
    "data = data.reset_index()\n",
    "data['isPandemic'] = data['date'].apply(lambda x: 1 if (datetime(2020,1,30) <= x) & (x <= datetime(2023,5,5)) else 0)\n",
    "data['isInvasion'] = data['date'].apply(lambda x: 1 if (x <= datetime.today()) else 0)\n",
    "data = data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictors = add_trend_season(data, [2,5,10,20,30,40,50])#+['isInvasion','isPandemic']\n",
    "test_scores = {}\n",
    "for i in range(250, 2000, 250):\n",
    "    train = data.iloc[0:i]\n",
    "    test  = data.iloc[i:i+250]\n",
    "    xgb.fit(train[predictors+new_predictors], train['target'])\n",
    "    test_scores.update({i: precision_score(test[target], xgb.predict(test[predictors+new_predictors]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ['T_0', 'T_1', 'T_2', 'T_3', 'T_4', 'T_5', 'T_6','T_7', 'T_8', 'T_9', 'T_10', 'T_11', 'T_12', 'T_13', 'T_14','T_15', 'T_16', 'T_17', 'T_18', 'T_19', 'T_20', 'T_21', 'T_22','T_23', 'T_24']:\n",
    "    if item in predictors:\n",
    "        predictors.remove(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train, test, predictors, model):\n",
    "    model.fit(train[predictors], train['target'])\n",
    "    preds = model.predict_proba(test[predictors])[:,1]\n",
    "    preds[preds >= 0.749582] = 1\n",
    "    preds[preds < 0.749582]  = 0\n",
    "    \n",
    "    preds = pd.Series(preds, index=test.index, name='Predictions')    \n",
    "    combined = pd.concat([test['target'], preds], axis=1)\n",
    "    return combined\n",
    "\n",
    "def backtest(X, predictors, model):\n",
    "    all_preds = []\n",
    "    for (train_idx, test_idx) in tss.split(X):\n",
    "        train = X.iloc[train_idx]\n",
    "        test  = X.iloc[test_idx]        \n",
    "        \n",
    "        preds = predict(train, test, predictors, model)\n",
    "        all_preds.append(preds)\n",
    "        \n",
    "    return model, pd.concat(all_preds)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=20, \n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.2,\n",
    "                    random_state=1,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits=3)\n",
    "dummy = data.copy()\n",
    "horizons = [2,7,14,50,90]\n",
    "new_predictors = add_trend_season(dummy, horizons)+['isPandemic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_xgb_model, preds = backtest(dummy, predictors+new_predictors, xgb)\n",
    "# precision = precision_score(preds['target'], preds['Predictions']) # 0.6226415094339622 precision\n",
    "# str_precision = str(precision).split('.')[1]\n",
    "# best_xgb_model.save_model(f'Models/CRYPTOmodels/xgb_{str_precision}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n",
      "Stopping after 1 empty pages\n"
     ]
    }
   ],
   "source": [
    "from DataPreparation.CryptoPreprocessor import CryptoPreprocessor\n",
    "from DataPreparation.TweetPreprocessor import TweetPreprocessor\n",
    "from DataPreparation.TextVectorizer import TextVectorizer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import LdaMulticore\n",
    "from TweetScraper import TweetScraper\n",
    "from xgboost import XGBClassifier\n",
    "from CryptoApi import CryptoApi\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "start = '2023-06-10'\n",
    "end = '2023-06-20'\n",
    "scrp = TweetScraper(start=start, end=end, max_empty_pages=1, max_workers=8)\n",
    "new_tweets = scrp.parallel_download_tweets()\n",
    "\n",
    "with open('crypto_token.txt','r') as f:\n",
    "    token = f.readline()\n",
    "\n",
    "crypto = CryptoApi(token)\n",
    "period_count = (datetime.strptime(end,'%Y-%m-%d')-datetime.strptime(start,'%Y-%m-%d')).days    \n",
    "new_btc = crypto.fetch_data('btc','usd', period='day', period_count=period_count)\n",
    "\n",
    "new_tweets = pd.DataFrame(new_tweets)\n",
    "tweets_df = pd.read_csv(r'Data/elon_tweets.csv', index_col=0)\n",
    "allowed_cols = [col for col in new_tweets.columns if col in tweets_df.columns]\n",
    "new_tweets = new_tweets[allowed_cols].copy()\n",
    "\n",
    "\n",
    "twt_prep = TweetPreprocessor(new_tweets)\n",
    "mod_tweets_df = twt_prep.transform()\n",
    "text2vec = TextVectorizer()\n",
    "preprocessing_pipeline = text2vec.make_pipeline()\n",
    "id2word, corpus = preprocessing_pipeline.transform(mod_tweets_df['rawContent'].values.tolist())\n",
    "\n",
    "temp_file = datapath(r\"D:\\Projects\\ElonMuskCrypto\\Models\\NLPmodels\\lda\")\n",
    "lda_model = LdaMulticore.load(temp_file)\n",
    "crypto_prep = CryptoPreprocessor()\n",
    "new_topics_btc = crypto_prep.transform(lda_model, mod_tweets_df, new_btc)\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.load_model('Models/CRYPTOmodels/xgb_6226415094339622.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_predictors = new_topics_btc[xgb_model.feature_names_in_].copy()\n",
    "predictions = xgb_model.predict(allowed_predictors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting new data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values set by a user\n",
    "start = '2023-06-10'\n",
    "end = '2023-06-20'\n",
    "\n",
    "scrp = TweetScraper(start=start, end=end, max_empty_pages=1, max_workers=8)\n",
    "new_tweets = scrp.parallel_download_tweets()\n",
    "\n",
    "with open('crypto_token.txt','r') as f:\n",
    "    token = f.readline()\n",
    "\n",
    "period_count = (datetime.strptime(end,'%Y-%m-%d')-datetime.strptime(start,'%Y-%m-%d')).days    \n",
    "crypto = CryptoApi(token)\n",
    "new_btc = crypto.fetch_data('btc','usd', period='day', period_count=period_count)\n",
    "\n",
    "new_tweets = pd.DataFrame(new_tweets)\n",
    "tweets_df = pd.read_csv(r'Data/elon_tweets.csv', index_col=0)\n",
    "allowed_cols = [col for col in new_tweets.columns if col in tweets_df.columns]\n",
    "new_tweets = new_tweets[allowed_cols].copy()\n",
    "\n",
    "\n",
    "twt_prep = TweetPreprocessor(new_tweets)\n",
    "mod_tweets_df = twt_prep.transform()\n",
    "\n",
    "\n",
    "# why tf do i need it? for updating my NLP model\n",
    "text2vec = TextVectorizer()\n",
    "preprocessing_pipeline = text2vec.make_pipeline()\n",
    "id2word, corpus = preprocessing_pipeline.transform(mod_tweets_df['rawContent'].values.tolist())\n",
    "\n",
    "\n",
    "temp_file = datapath(r\"D:\\Projects\\ElonMuskCrypto\\Models\\NLPmodels\\lda\")\n",
    "lda_model = LdaMulticore.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_prep = CryptoPreprocessor()\n",
    "new_topics_btc = crypto_prep.transform(lda_model, mod_tweets_df, new_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model.load_model('Models/CRYPTOmodels/xgb_6226415094339622.json')\n",
    "\n",
    "new_topics_btc[\"tomorrow\"] = new_topics_btc[\"close\"].shift(-1)\n",
    "new_topics_btc[\"target\"] = (new_topics_btc[\"tomorrow\"] > new_topics_btc[\"close\"]).astype(int)\n",
    "new_topics_btc = new_topics_btc.reset_index()\n",
    "new_topics_btc['isPandemic'] = new_topics_btc['date'].apply(lambda x: 1 if (datetime(2020,1,30) <= x) & (x <= datetime(2023,5,5)) else 0)\n",
    "new_topics_btc = new_topics_btc.set_index('date')\n",
    "\n",
    "horizons = [2,7,14,50,90]\n",
    "new_predictors = add_trend_season(new_topics_btc, horizons)+['isPandemic']\n",
    "predict_data = new_topics_btc[xgb_model.feature_names_in_].copy()\n",
    "\n",
    "\n",
    "predictions = xgb_model.predict(predict_data)\n",
    "precision_score(new_topics_btc['target'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots(1)\n",
    "# fig.set_figwidth(20)\n",
    "# fig.set_figheight(10)\n",
    "\n",
    "# dummy = data.copy()\n",
    "# horizons = [2,7,14,50,90]\n",
    "# new_predictors = add_trend_season(dummy, horizons)+['isPandemic']\n",
    "\n",
    "# for days in range(30,210,30):\n",
    "#     xgb = XGBClassifier(n_estimators=40, \n",
    "#                         max_depth=4,\n",
    "#                         learning_rate=0.1,\n",
    "#                         random_state=1,\n",
    "#                         n_jobs=-1)\n",
    "    \n",
    "#     test_scores  = {}\n",
    "#     train_scores = {} \n",
    "\n",
    "#     for i in range(0, 2000, days):\n",
    "#         train = dummy.iloc[0:i]\n",
    "#         test  = dummy.iloc[i:i+days]\n",
    "#         xgb.fit(train[predictors+new_predictors], train['target'])\n",
    "#         test_scores.update({i: precision_score(test[target], xgb.predict(test[predictors+new_predictors]))})\n",
    "#         train_scores.update({i: precision_score(train[target], xgb.predict(train[predictors+new_predictors]))})\n",
    "\n",
    "#     ax1.plot(test_scores.keys(), test_scores.values(), label=f'{days}')\n",
    "#     ax1.scatter(test_scores.keys(), test_scores.values())\n",
    "#     ax1.axhline(0.5, c='red')\n",
    "#     ax1.grid(axis='y')\n",
    "#     ax1.legend()\n",
    "\n",
    "#     mean_test_accuracy = np.mean(list(test_scores.values()))\n",
    "#     mean_train_accuracy = np.mean(list(train_scores.values()))\n",
    "#     std_accuracy  = np.std(list(test_scores.values()))\n",
    "#     print(f'{days} = {round(mean_test_accuracy, 5):<6} | {round(mean_train_accuracy, 5):<6} | {round(std_accuracy, 5)}')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(data['close'])\n",
    "plt.plot(data[(datetime(2020,11,30) <= data.index) & (data.index <= datetime(2022,6,30))]['close'])\n",
    "# plt.plot(data[(datetime(2023,2,24) <= data.index) & (data.index <= datetime.today())]['close'], c='purple')\n",
    "plt.plot(data['close'].iloc[-350:], c='pink')\n",
    "\n",
    "\n",
    "plt.plot(data['close'].rolling(125).mean(), color='lime')\n",
    "\n",
    "# add columns for \"crypto boom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "df = pd.DataFrame(zip(xgb.feature_names_in_, xgb.feature_importances_))\n",
    "\n",
    "\n",
    "plt.bar(df[0], df[1], edgecolor='black')\n",
    "# plt.margins(0,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_df = pd.read_csv('Data/eth_data.csv')\n",
    "eth_df = eth_df.rename(columns={'time':'date'}).drop(['Unnamed: 0','conversionType','conversionSymbol'], axis=1)\n",
    "eth_df['date'] = eth_df['date'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "eth_df = eth_df.set_index('date')\n",
    "\n",
    "\n",
    "doge_df = pd.read_csv('Data/doge_data.csv')\n",
    "doge_df = doge_df.rename(columns={'time':'date'}).drop(['Unnamed: 0','conversionType','conversionSymbol'], axis=1)\n",
    "doge_df['date'] = doge_df['date'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "doge_df = doge_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for crypto, name in zip([topics_btc, eth_df, doge_df], ['btc','eth','doge']):\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(crypto['close'].values.reshape(-1, 1))    \n",
    "    plt.plot(data, label=name)\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# train_data = btc_df[:400].copy()\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaled_data = scaler.fit_transform(train_data['close'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "# prediction_days = 60\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "# for x in range(prediction_days, len(scaled_data)):\n",
    "#     x_train.append(scaled_data[x-prediction_days:x, 0])\n",
    "#     y_train.append(scaled_data[x, 0])\n",
    "    \n",
    "# x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "\n",
    "# test_data = btc_df[400:].copy()\n",
    "# total_dataset = pd.concat((btc_df['close'], test_data['close']))\n",
    "\n",
    "# model_inputs = total_dataset[len(total_dataset) - len(test_data) - prediction_days:].values\n",
    "# model_inputs = model_inputs.reshape(-1,1)\n",
    "# model_inputs = scaler.transform(model_inputs)\n",
    "\n",
    "# X_test = []\n",
    "# for x in range(prediction_days, len(model_inputs)):\n",
    "#     X_test.append(model_inputs[x-prediction_days:x, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (data['target'].value_counts()/len(data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make tracker for features that were dropped due to high sparsity in case of leaving threshold of 50% sparsity\n",
    "# for future model upgrading and adding new features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
