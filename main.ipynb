{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetching crypto and tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from crypto_api import CryptoApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "#btc_df = pd.read_csv(r'Data\\btc_data.csv',usecols=lambda x: x != \"Unnamed: 0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: GENERAL DATA TASKS:\n",
    "0) find a way to deal with multiple tweets for a day\n",
    "1) merge 2 datasets into \n",
    "2) imput missing data, maybe try interpolation or expectation maximization\n",
    "    2.1) compare with mean, median imput methods\n",
    "3) ivestigate relationship within data, maybe correlation matrix etc\n",
    "'''\n",
    "\n",
    "tweets_df = pd.read_csv(r'Data/elon_tweets.csv', index_col=0)\n",
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "                                                \n",
    "tweets_df = (tweets_df\n",
    "             .dropna(axis=1, how='all')\n",
    "             .drop(['vibe','cashtags'], axis=1)) # 1 and 18 notna values respectively  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with sparse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cols = tweets_df.columns[tweets_df.notnull().mean() < 1.0].values.copy()\n",
    "\n",
    "mod_tweets_df = tweets_df.copy()\n",
    "mod_tweets_df = (mod_tweets_df[mod_tweets_df['lang']=='en']\n",
    "                 .drop(['id','url','source','sourceUrl'], axis=1)                 \n",
    "                 .reset_index(drop=True)\n",
    "                 .copy())\n",
    "\n",
    "mod_tweets_df = mod_tweets_df.drop(['lang'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "mod_tweets_df['sourceLabel_encoded'] = encoder.fit_transform(mod_tweets_df['sourceLabel'].values.reshape(-1, 1))\n",
    "mod_tweets_df['isReplied']   = [0 if type(tweet)==float else 1 for tweet in mod_tweets_df['inReplyToUser']]\n",
    "mod_tweets_df['isMentioned'] = [0 if type(tweet)==float else 1 for tweet in mod_tweets_df['mentionedUsers']]\n",
    "\n",
    "#mod_tweets_df = mod_tweets_df.drop(['sourceLabel','inReplyToUser','mentionedUsers'], axis=1)\n",
    "\n",
    "\n",
    "def extract_dict(line: str, prepare_to_df: False):\n",
    "    \"\"\"Extracts data from a dict represented as string and makes it a dict.\n",
    "\n",
    "    ## Parameters:\n",
    "        line (str): row of a Series/DataFrame to be preprocessed.\n",
    "        prepare_to_df (bool): prepares extracted dict to be wrapped into DataFrame.\n",
    "\n",
    "    ## Returns:\n",
    "        dict: extracted dict from string.\n",
    "    \"\"\"    \n",
    "\n",
    "    extracted_content = dict(re.findall(r\"'(\\w+)': '?({.*}|datetime.datetime\\(.*\\)|[\\w\\d/:\\. ]*)'?\", line))\n",
    "    \n",
    "    # Wraps dict values into lists to be easily represented as a DataFrame row.\n",
    "    if prepare_to_df:\n",
    "        for key,value in extracted_content.items():\n",
    "            if value == '':\n",
    "                extracted_content[key] = [None]\n",
    "            else:\n",
    "                extracted_content[key] = value\n",
    "        \n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "new_df = mod_tweets_df.copy()     \n",
    "extracted_df = (pd.DataFrame([*mod_tweets_df['user']\n",
    "                              .apply(lambda x: extract_dict(x, True))])\n",
    "                )\n",
    "# TODO: try this json approach, but replace datetime via regex to str and then upcast to datetime again\n",
    "# my_json = my_json.replace(\"'\",'\"').replace('None','null')\n",
    "# json.loads(my_json)\n",
    "\n",
    "new_df = (pd.concat([new_df, extracted_df], axis=1)\n",
    "            .drop(['user','username','id','displayname','verified','created',\n",
    "                    'location','protected','profileImageUrl','profileBannerUrl',\n",
    "                    'rawDescription','renderedDescription','favouritesCount',\n",
    "                    'friendsCount','mediaCount','statusesCount'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting columns containing numbers to int after extraction.\n",
    "for column in new_df:\n",
    "    if 'Count' in column:\n",
    "        new_df[column] = new_df[column].astype('Int64').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "new_df[['rawContent','isReplied','isMentioned']].query(\"rawContent.str.contains('@')\")\n",
    "\n",
    "new_df['mentionsCount'] = new_df['rawContent'].str.count(r'@[\\w\\d]+')\n",
    "new_df['mentions'] = new_df['rawContent'].apply(lambda x : re.findall(r'(@[^\\s]+)', x))\n",
    "\n",
    "count = 0\n",
    "for a,b in new_df[['mentionsCount','mentions']].values:\n",
    "    if a==len(b):\n",
    "        count +=1 \n",
    "print(count==len(new_df))\n",
    "\n",
    "new_df['charCount'] = new_df['rawContent'].apply(lambda x: len(x))\n",
    "new_df = new_df.drop('descriptionLinks', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'url\\': \\'https://twitter.com/ggreenwald/status/1625871270737809408\\', \\'date\\': datetime.datetime(2023, 2, 15, 14, 54, 52, tzinfo=datetime.timezone.utc), \\'rawContent\\': \"The corporate media\\'s ability to -- overnight -- turn anyone who dissents in anyway into some sort of fascist or even Hitler-like figure, and then have millions of their followers go around mindlessly repeating it, is both impressive and chilling:\", \\'renderedContent\\': \"The corporate media\\'s ability to -- overnight -- turn anyone who dissents in anyway into some sort of fascist or even Hitler-like figure, and then have millions of their followers go around mindlessly repeating it, is both impressive and chilling:\", \\'id\\': 1625871270737809408, \\'user\\': {\\'username\\': \\'ggreenwald\\', \\'id\\': 16076032, \\'displayname\\': \\'Glenn Greenwald\\', \\'rawDescription\\': \\'Journalist; Author; Host, @SystemUpdate_; Columnist, @Folha; Co-Founder: The Intercept, @TheInterceptBr; @abrigo_hope, @FreedomofPress, @ongcriadefavela. Vegan.\\', \\'renderedDescription\\': \\'Journalist; Author; Host, @SystemUpdate_; Columnist, @Folha; Co-Founder: The Intercept, @TheInterceptBr; @abrigo_hope, @FreedomofPress, @ongcriadefavela. Vegan.\\', \\'descriptionLinks\\': None, \\'verified\\': True, \\'created\\': datetime.datetime(2008, 9, 1, 3, 13, 32, tzinfo=datetime.timezone.utc), \\'followersCount\\': 2005848, \\'friendsCount\\': 981, \\'statusesCount\\': 89311, \\'favouritesCount\\': 20442, \\'listedCount\\': 22110, \\'mediaCount\\': 10051, \\'location\\': \\'GlennGreenwald@gmail.com\\', \\'protected\\': False, \\'link\\': {\\'text\\': \\'linktr.ee/glenngreenwald\\', \\'url\\': \\'https://linktr.ee/glenngreenwald\\', \\'tcourl\\': \\'https://t.co/rw7Uw16ZvA\\', \\'indices\\': (0, 23)}, \\'profileImageUrl\\': \\'https://pbs.twimg.com/profile_images/1092582027994509312/cpYWuYI9_normal.jpg\\', \\'profileBannerUrl\\': \\'https://pbs.twimg.com/profile_banners/16076032/1670619705\\', \\'label\\': None}, \\'replyCount\\': 1814, \\'retweetCount\\': 8513, \\'likeCount\\': 40283, \\'quoteCount\\': 585, \\'conversationId\\': 1625871270737809408, \\'lang\\': \\'en\\', \\'source\\': \\'<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>\\', \\'sourceUrl\\': \\'https://mobile.twitter.com\\', \\'sourceLabel\\': \\'Twitter Web App\\', \\'links\\': None, \\'media\\': None, \\'retweetedTweet\\': None, \\'quotedTweet\\': None, \\'inReplyToTweetId\\': None, \\'inReplyToUser\\': None, \\'mentionedUsers\\': None, \\'coordinates\\': None, \\'place\\': None, \\'hashtags\\': None, \\'cashtags\\': None, \\'card\\': None, \\'viewCount\\': 76140755, \\'vibe\\': None}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#links = tweets_df[tweets_df.columns[tweets_df.columns.isin(new_df.columns)==False]]['links'].value_counts().copy()\n",
    "\n",
    "tweets_df[tweets_df['rawContent']=='True']['quotedTweet'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "def clean_text(raw_text):    \n",
    "    cleaned_text = re.sub(r' \\'?(displayname|renderedDescription)\\'?: (.*?)(\\'|None),', '', raw_text)\n",
    "    cleaned_text = (cleaned_text\n",
    "                    .replace(\"'\",'\"')\n",
    "                    .replace('None','null')\n",
    "                    .replace('True','true')\n",
    "                    .replace('False','false'))\n",
    "    # cleaned_text = re.sub(r'(\\w+)\"(\\w+)', r\"\\1'\\2\", cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def deserialize(text):    \n",
    "    deserialized_texts = []\n",
    "    extract_dicts = re.findall(r'{.*?}',text)\n",
    "    \n",
    "    for str_dict in extract_dicts:\n",
    "        cleaned_text = clean_text(str_dict)\n",
    "\n",
    "        pattern = r'datetime.datetime\\(.*\\)'\n",
    "        cleaned_text = re.sub(f'({pattern})',r'\"\\1\"',cleaned_text)\n",
    "        \n",
    "        deserialized_text = json.loads(cleaned_text)\n",
    "        \n",
    "        if deserialized_text['created']!=None:\n",
    "            deserialized_text['created'] = eval(deserialized_text['created'])\n",
    "        \n",
    "        deserialized_texts.append(deserialized_text)\n",
    "\n",
    "    return deserialized_texts\n",
    "    \n",
    "    \n",
    "new_df = new_df.drop('inReplyToUser',axis=1)\n",
    "new_df['mentionedUsers'] = new_df['mentionedUsers'].apply(lambda x: deserialize(x) if type(x)==str else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rawContent',\n",
       " 'renderedContent',\n",
       " 'sourceLabel',\n",
       " 'links',\n",
       " 'media',\n",
       " 'quotedTweet',\n",
       " 'mentionedUsers',\n",
       " 'hashtags',\n",
       " 'card',\n",
       " 'link',\n",
       " 'label',\n",
       " 'mentions']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_cols = [col for col in new_df if new_df[col].dtype==object]\n",
    "object_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39        ['whatcouldpossiblygowrong']\n",
       "95                      ['PlutoStamp']\n",
       "107                     ['antarctica']\n",
       "133                    ['APSpaceChat']\n",
       "134      ['OccupyMars', 'APSpaceChat']\n",
       "135                    ['APSpaceChat']\n",
       "149                   ['dragonlaunch']\n",
       "150                         ['Dragon']\n",
       "152                   ['dragonlaunch']\n",
       "153                   ['DragonLaunch']\n",
       "154                   ['DragonLaunch']\n",
       "158                   ['DragonLaunch']\n",
       "159                   ['dragonlaunch']\n",
       "161             ['GrasshopperProject']\n",
       "163                         ['Dragon']\n",
       "166                         ['Dragon']\n",
       "167                         ['Dragon']\n",
       "185                     ['OccupyMars']\n",
       "259                        ['Climate']\n",
       "270               ['AwesomeXmasGifts']\n",
       "276                  ['Zeitgeist2012']\n",
       "305                           ['SB47']\n",
       "345                         ['Dragon']\n",
       "353                           ['SXSW']\n",
       "357                           ['SXSW']\n",
       "398                        ['TeslaTX']\n",
       "399                         ['hhgttg']\n",
       "607               ['KatieWoodencloak']\n",
       "712                           ['Yutu']\n",
       "2982                   ['FalconHeavy']\n",
       "2983                   ['FalconHeavy']\n",
       "2984                   ['FalconHeavy']\n",
       "2985         ['FalconHeavy', 'SpaceX']\n",
       "3597        ['ThrowFlamesResponsibly']\n",
       "5968                     ['moneygang']\n",
       "6762                  ['Armageddon69']\n",
       "7429                ['DeleteFacebook']\n",
       "8431              ['JusticeForGeorge']\n",
       "10494           ['resistanceisfutile']\n",
       "10682                         ['Mars']\n",
       "15015                   ['FreeSpeech']\n",
       "Name: hashtags, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS IS IMPORTANT AF\n",
    "new_df['hashtags'][new_df['hashtags'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'title': 'National Geographic', 'url': 'http://ow.ly/877EB', 'description': 'Explore National Geographic. A world leader in geography, cartography and exploration.', 'thumbnailUrl': 'https://pbs.twimg.com/card_img/1643958155918073856/hClZnWZr?format=jpg&name=orig', 'siteUser': {'username': 'NatGeo', 'id': 17471979, 'displayname': 'National Geographic', 'rawDescription': 'Taking our understanding and awareness of the world further for more than 130 years', 'renderedDescription': 'Taking our understanding and awareness of the world further for more than 130 years', 'descriptionLinks': None, 'verified': True, 'created': datetime.datetime(2008, 11, 18, 21, 28, 10, tzinfo=datetime.timezone.utc), 'followersCount': 28849980, 'friendsCount': 196, 'statusesCount': 69663, 'favouritesCount': 7873, 'listedCount': 64442, 'mediaCount': 7397, 'location': 'Global', 'protected': False, 'link': {'text': 'nationalgeographic.com', 'url': 'http://www.nationalgeographic.com', 'tcourl': 'https://t.co/1kQE47ceZW', 'indices': (0, 23)}, 'profileImageUrl': 'https://pbs.twimg.com/profile_images/1592560747707600897/6_8AjTmK_normal.jpg', 'profileBannerUrl': 'https://pbs.twimg.com/profile_banners/17471979/1674253771', 'label': None}, 'creatorUser': None}\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['card'][new_df['card'].notna()].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'description': 'Twitter', 'url': 'https://twitter.com/Twitter', 'badgeUrl': 'https://pbs.twimg.com/profile_images/1488548719062654976/u6qfBBkF_bigger.jpg', 'longDescription': None}}\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['label'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url                True\n",
      "date               True\n",
      "rawContent         True\n",
      "renderedContent    True\n",
      "id                 True\n",
      "user               True\n",
      "replyCount         True\n",
      "retweetCount       True\n",
      "likeCount          True\n",
      "quoteCount         True\n",
      "conversationId     True\n",
      "lang               True\n",
      "source             True\n",
      "sourceUrl          True\n",
      "sourceLabel        True\n",
      "links              True\n",
      "media              True\n",
      "quotedTweet        True\n",
      "inReplyToTweetId   True\n",
      "inReplyToUser      True\n",
      "mentionedUsers     True\n",
      "hashtags           True\n",
      "card               True\n",
      "viewCount          True\n"
     ]
    }
   ],
   "source": [
    "# new_df['sourceLabel'] encode this 1,2,3\n",
    "new_df = new_df.drop(['links','media','link'], axis=1)\n",
    "new_df['quotedTweet'][new_df['quotedTweet'].notna()].iloc[0]\n",
    "\n",
    "for item in tweets_df.columns:\n",
    "    print(f\"{item:<18} {item in new_df['quotedTweet'][new_df['quotedTweet'].notna()].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'url\\': \\'https://twitter.com/SpaceX/status/614551591601844224\\', \\'date\\': datetime.datetime(2015, 6, 26, 21, 51, 31, tzinfo=datetime.timezone.utc), \\'rawContent\\': \\'Rocket completed hold down firing today in advance of Sunday cargo mission to @Space_Station http://t.co/tdni53IviI http://t.co/PjiYSIDTOu\\', \\'renderedContent\\': \\'Rocket completed hold down firing today in advance of Sunday cargo mission to @Space_Station spacex.com/webcast http://t.co/PjiYSIDTOu\\', \\'id\\': 614551591601844224, \\'user\\': {\\'username\\': \\'SpaceX\\', \\'id\\': 34743251, \\'displayname\\': \\'SpaceX\\', \\'rawDescription\\': \\'SpaceX designs, manufactures and launches the world’s most advanced rockets and spacecraft\\', \\'renderedDescription\\': \\'SpaceX designs, manufactures and launches the world’s most advanced rockets and spacecraft\\', \\'descriptionLinks\\': None, \\'verified\\': True, \\'created\\': datetime.datetime(2009, 4, 23, 21, 53, 30, tzinfo=datetime.timezone.utc), \\'followersCount\\': 28768901, \\'friendsCount\\': 100, \\'statusesCount\\': 6751, \\'favouritesCount\\': 411, \\'listedCount\\': 39531, \\'mediaCount\\': 2281, \\'location\\': \\'Hawthorne, CA\\', \\'protected\\': False, \\'link\\': {\\'text\\': \\'spacex.com\\', \\'url\\': \\'http://www.spacex.com\\', \\'tcourl\\': \\'https://t.co/h50T0bSAeC\\', \\'indices\\': (0, 23)}, \\'profileImageUrl\\': \\'https://pbs.twimg.com/profile_images/1082744382585856001/rH_k3PtQ_normal.jpg\\', \\'profileBannerUrl\\': \\'https://pbs.twimg.com/profile_banners/34743251/1677862631\\', \\'label\\': None}, \\'replyCount\\': 38, \\'retweetCount\\': 396, \\'likeCount\\': 597, \\'quoteCount\\': 1, \\'conversationId\\': 614551591601844224, \\'lang\\': \\'en\\', \\'source\\': \\'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>\\', \\'sourceUrl\\': \\'http://twitter.com\\', \\'sourceLabel\\': \\'Twitter Web Client\\', \\'links\\': [{\\'text\\': \\'spacex.com/webcast\\', \\'url\\': \\'http://www.spacex.com/webcast\\', \\'tcourl\\': \\'http://t.co/tdni53IviI\\', \\'indices\\': (93, 115)}], \\'media\\': [{\\'previewUrl\\': \\'https://pbs.twimg.com/media/CIdTalVVAAAEcqv?format=jpg&name=small\\', \\'fullUrl\\': \\'https://pbs.twimg.com/media/CIdTalVVAAAEcqv?format=jpg&name=orig\\', \\'altText\\': None}], \\'retweetedTweet\\': None, \\'quotedTweet\\': None, \\'inReplyToTweetId\\': None, \\'inReplyToUser\\': None, \\'mentionedUsers\\': [{\\'username\\': \\'Space_Station\\', \\'id\\': 1451773004, \\'displayname\\': \\'International Space Station\\', \\'rawDescription\\': None, \\'renderedDescription\\': None, \\'descriptionLinks\\': None, \\'verified\\': None, \\'created\\': None, \\'followersCount\\': None, \\'friendsCount\\': None, \\'statusesCount\\': None, \\'favouritesCount\\': None, \\'listedCount\\': None, \\'mediaCount\\': None, \\'location\\': None, \\'protected\\': None, \\'link\\': None, \\'profileImageUrl\\': None, \\'profileBannerUrl\\': None, \\'label\\': None}], \\'coordinates\\': None, \\'place\\': None, \\'hashtags\\': None, \\'cashtags\\': None, \\'card\\': None, \\'viewCount\\': None, \\'vibe\\': None}'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['quotedTweet'][tweets_df['quotedTweet'].notna()].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Model Bulding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, matutils, utils, models\n",
    "import spacy\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    url_pattern = [{\"label\": \"URL\",\n",
    "                    \"pattern\": [{\"LIKE_URL\": True}]}]\n",
    "\n",
    "    ruler = nlp.add_pipe('entity_ruler', before='ner')\n",
    "    ruler.add_patterns(url_pattern)\n",
    "    \n",
    "    texts_out = []\n",
    "    if type(texts)!=list:\n",
    "        texts = [texts]\n",
    "    \n",
    "    for text in texts:\n",
    "        # TODO: consider using nlp.pipe which should be faster\n",
    "        doc = nlp(text)\n",
    "        cleaned_text = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_ != 'URL' and not token.is_stop and token.pos_ in allowed_postags:\n",
    "                cleaned_text.append(token.lemma_)\n",
    "        final = ' '.join(cleaned_text)\n",
    "        texts_out.append(final)\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def create_ngrams(texts):\n",
    "    data_words = []\n",
    "    for text in texts:\n",
    "        new = utils.simple_preprocess(text)\n",
    "        data_words.append(new)\n",
    "\n",
    "    bigrams_phrases  = models.Phrases(data_words, min_count=3, threshold=50)\n",
    "    trigrams_phrases = models.Phrases(bigrams_phrases[data_words], threshold=50)\n",
    "\n",
    "    bigram  = models.phrases.Phraser(bigrams_phrases)\n",
    "    trigram = models.phrases.Phraser(trigrams_phrases)\n",
    "\n",
    "    data_bigrams = [bigram[doc] for doc in data_words]\n",
    "    data_bigrams_trigrams = [trigram[bigram[doc]] for doc in data_bigrams]\n",
    "    \n",
    "    return data_bigrams_trigrams\n",
    "\n",
    "\n",
    "def vectorize_texts(texts_ngrams):\n",
    "    id2word = corpora.Dictionary(texts_ngrams)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts_ngrams]\n",
    "    return id2word, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def make_custom_pipeline(steps):\n",
    "    for i, step in enumerate(steps):\n",
    "        steps.insert(i, (step[0], FunctionTransformer(step[1])))\n",
    "        steps.remove(step)\n",
    "\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "# with open('lemmatized_texts.txt', 'r', encoding=\"utf-8\") as f:\n",
    "#     lemmatized_texts = f.readlines()\n",
    "    \n",
    "# lemmatized_texts = [line.replace('\\n','') for line in lemmatized_texts]\n",
    "\n",
    "# lemmatized_texts = lemmatization(new_df['rawContent'])\n",
    "steps = [('lemmatization', lemmatization),\n",
    "         ('trigrams', create_ngrams),\n",
    "         ('vectorization', vectorize_texts)]\n",
    "\n",
    "preprocessing_pipeline = make_custom_pipeline(steps)\n",
    "id2word, corpus = preprocessing_pipeline.transform(new_df['rawContent'].values.tolist())\n",
    "\n",
    "# data_bigrams_trigrams = create_ngrams(lemmatized_texts)\n",
    "# id2word, corpus = vectorize_texts(data_bigrams_trigrams)\n",
    "\n",
    "#tfidf = models.TfidfModel(corpus, id2word=id2word)\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.6,\n",
    "#                                    min_df=5,\n",
    "#                                    ngram_range=(1,3))\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "# corpus = matutils.Sparse2Corpus(tfidf_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_value = 0.03\n",
    "# words = []\n",
    "# words_missing_in_tfidf = []\n",
    "\n",
    "# for i in range(0, len(corpus)):\n",
    "#     bow = corpus[i]\n",
    "#     low_value_words = []\n",
    "#     tfidf_ids = [id for id,_ in tfidf[bow]]\n",
    "#     bow_idf = [id for id,_ in bow]\n",
    "#     low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "#     drops = low_value_words+words_missing_in_tfidf\n",
    "    \n",
    "#     for item in drops:\n",
    "#         words.append(id2word[item])\n",
    "    \n",
    "#     # words with tfidf score of 0 will be missing\n",
    "#     words_missing_in_tfidf = [id for id in bow_idf if id not in tfidf_ids]\n",
    "#     new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "#     corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "def bayesian_tuning(model, params_grid: dict, texts, verbose=False):\n",
    "    # TODO: implement bayesian tuning\n",
    "    models_scores = {}\n",
    "    for i in range(95,170,5):\n",
    "        lda_model = model(corpus=params_grid['corpus'],\n",
    "                          num_topics=i,\n",
    "                          id2word=params_grid['id2word'],\n",
    "                          random_state=1,\n",
    "                          passes=10,\n",
    "                          per_word_topics=True)\n",
    "        \n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                             texts=texts, \n",
    "                                             corpus=params_grid['corpus'], \n",
    "                                             dictionary=params_grid['id2word']\n",
    "                                             )\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        \n",
    "        models_scores.update({lda_model: coherence_score})\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Topics {i:<3}: {coherence_score}')\n",
    "    \n",
    "    return models_scores\n",
    "\n",
    "\n",
    "params_grid = {'corpus':corpus,  \n",
    "               'num_topics':10, \n",
    "               'id2word':id2word, \n",
    "               'random_state':1, \n",
    "               'update_every':1, \n",
    "               'chunksize':3000, \n",
    "               'passes':2}\n",
    "\n",
    "# lda_models_scores = bayesian_tuning(LdaMulticore, params_grid, texts, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models import LdaMulticore\n",
    "import pyLDAvis\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         num_topics=20,\n",
    "                         id2word=id2word,\n",
    "                         random_state=1,\n",
    "                         passes=10,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_new_texts(texts: list[str]):\n",
    "    steps = [('lemmatization', lemmatization),\n",
    "            ('trigrams', create_ngrams)]\n",
    "\n",
    "    corpus_pipeline = make_custom_pipeline(steps)\n",
    "    texts_ngrams = corpus_pipeline.transform(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts_ngrams]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [id2word.doc2bow(text) for text in texts_ngrams]\n",
    "# lda_model.get_document_topics(corpus, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['vectorized']  = preprocessing_pipeline.transform(new_df['rawContent'].values.tolist())[1]\n",
    "new_df['TopicsProbs'] = new_df['vectorized'].apply(lambda x: dict(lda_model.get_document_topics(x, minimum_probability=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['TopicsProbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = pd.read_csv('Data/btc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df.iloc[0:1, 1:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesCount = new_df.groupby(new_df['date'].dt.date)['date'].count()\n",
    "new_df = new_df.merge(datesCount.rename('tweetsCount'), left_on=new_df['date'].dt.date, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_this = new_df.groupby(new_df['date'].dt.date).count()['date']\n",
    "plot_this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
