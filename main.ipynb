{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetching crypto and tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from crypto_api import CryptoApi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "#btc_df = pd.read_csv(r'Data\\btc_data.csv',usecols=lambda x: x != \"Unnamed: 0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: GENERAL DATA TASKS:\n",
    "0) find a way to deal with multiple tweets for a day\n",
    "1) merge 2 datasets into \n",
    "2) imput missing data, maybe try interpolation or expectation maximization\n",
    "    2.1) compare with mean, median imput methods\n",
    "3) ivestigate relationship within data, maybe correlation matrix etc\n",
    "'''\n",
    "\n",
    "tweets_df = pd.read_csv(r'Data/elon_tweets.csv', index_col=0)\n",
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "                                                \n",
    "tweets_df = (tweets_df\n",
    "             .dropna(axis=1, how='all')\n",
    "             .drop(['vibe','cashtags'], axis=1)) # 1 and 18 notna values respectively  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with sparse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cols = tweets_df.columns[tweets_df.notnull().mean() < 1.0].values.copy()\n",
    "\n",
    "mod_tweets_df = tweets_df.copy()\n",
    "mod_tweets_df = (mod_tweets_df[mod_tweets_df['lang']=='en']\n",
    "                 .drop(['id','url','source','sourceUrl'], axis=1)                 \n",
    "                 .reset_index(drop=True)\n",
    "                 .copy())\n",
    "\n",
    "mod_tweets_df = mod_tweets_df.drop(['lang'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "mod_tweets_df['sourceLabel_encoded'] = encoder.fit_transform(mod_tweets_df['sourceLabel'].values.reshape(-1, 1))\n",
    "mod_tweets_df['isReplied']   = [0 if type(tweet)==float else 1 for tweet in mod_tweets_df['inReplyToUser']]\n",
    "mod_tweets_df['isMentioned'] = [0 if type(tweet)==float else 1 for tweet in mod_tweets_df['mentionedUsers']]\n",
    "\n",
    "#mod_tweets_df = mod_tweets_df.drop(['sourceLabel','inReplyToUser','mentionedUsers'], axis=1)\n",
    "\n",
    "\n",
    "def extract_dict(line: str, prepare_to_df: False):\n",
    "    \"\"\"Extracts data from a dict represented as string and makes it a dict.\n",
    "\n",
    "    ## Parameters:\n",
    "        line (str): row of a Series/DataFrame to be preprocessed.\n",
    "        prepare_to_df (bool): prepares extracted dict to be wrapped into DataFrame.\n",
    "\n",
    "    ## Returns:\n",
    "        dict: extracted dict from string.\n",
    "    \"\"\"    \n",
    "\n",
    "    extracted_content = dict(re.findall(r\"'(\\w+)': '?({.*}|datetime.datetime\\(.*\\)|[\\w\\d/:\\. ]*)'?\", line))\n",
    "    \n",
    "    # Wraps dict values into lists to be easily represented as a DataFrame row.\n",
    "    if prepare_to_df:\n",
    "        for key,value in extracted_content.items():\n",
    "            if value == '':\n",
    "                extracted_content[key] = [None]\n",
    "            else:\n",
    "                extracted_content[key] = value\n",
    "        \n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "new_df = mod_tweets_df.copy()     \n",
    "extracted_df = (pd.DataFrame([*mod_tweets_df['user']\n",
    "                              .apply(lambda x: extract_dict(x, True))])\n",
    "                )\n",
    "# TODO: try this json approach, but replace datetime via regex to str and then upcast to datetime again\n",
    "# my_json = my_json.replace(\"'\",'\"').replace('None','null')\n",
    "# json.loads(my_json)\n",
    "\n",
    "new_df = (pd.concat([new_df, extracted_df], axis=1)\n",
    "            .drop(['user','username','id','displayname','verified','created',\n",
    "                    'location','protected','profileImageUrl','profileBannerUrl',\n",
    "                    'rawDescription','renderedDescription','favouritesCount',\n",
    "                    'friendsCount','mediaCount','statusesCount'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting columns containing numbers to int after extraction.\n",
    "for column in new_df:\n",
    "    if 'Count' in column:\n",
    "        new_df[column] = new_df[column].astype('Int64').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "new_df[['rawContent','isReplied','isMentioned']].query(\"rawContent.str.contains('@')\")\n",
    "\n",
    "new_df['mentionsCount'] = new_df['rawContent'].str.count(r'@[\\w\\d]+')\n",
    "new_df['mentions'] = new_df['rawContent'].apply(lambda x : re.findall(r'(@[^\\s]+)', x))\n",
    "\n",
    "count = 0\n",
    "for a,b in new_df[['mentionsCount','mentions']].values:\n",
    "    if a==len(b):\n",
    "        count +=1 \n",
    "print(count==len(new_df))\n",
    "\n",
    "new_df['charCount'] = new_df['rawContent'].apply(lambda x: len(x))\n",
    "new_df = new_df.drop('descriptionLinks', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'url\\': \\'https://twitter.com/ggreenwald/status/1625871270737809408\\', \\'date\\': datetime.datetime(2023, 2, 15, 14, 54, 52, tzinfo=datetime.timezone.utc), \\'rawContent\\': \"The corporate media\\'s ability to -- overnight -- turn anyone who dissents in anyway into some sort of fascist or even Hitler-like figure, and then have millions of their followers go around mindlessly repeating it, is both impressive and chilling:\", \\'renderedContent\\': \"The corporate media\\'s ability to -- overnight -- turn anyone who dissents in anyway into some sort of fascist or even Hitler-like figure, and then have millions of their followers go around mindlessly repeating it, is both impressive and chilling:\", \\'id\\': 1625871270737809408, \\'user\\': {\\'username\\': \\'ggreenwald\\', \\'id\\': 16076032, \\'displayname\\': \\'Glenn Greenwald\\', \\'rawDescription\\': \\'Journalist; Author; Host, @SystemUpdate_; Columnist, @Folha; Co-Founder: The Intercept, @TheInterceptBr; @abrigo_hope, @FreedomofPress, @ongcriadefavela. Vegan.\\', \\'renderedDescription\\': \\'Journalist; Author; Host, @SystemUpdate_; Columnist, @Folha; Co-Founder: The Intercept, @TheInterceptBr; @abrigo_hope, @FreedomofPress, @ongcriadefavela. Vegan.\\', \\'descriptionLinks\\': None, \\'verified\\': True, \\'created\\': datetime.datetime(2008, 9, 1, 3, 13, 32, tzinfo=datetime.timezone.utc), \\'followersCount\\': 2005848, \\'friendsCount\\': 981, \\'statusesCount\\': 89311, \\'favouritesCount\\': 20442, \\'listedCount\\': 22110, \\'mediaCount\\': 10051, \\'location\\': \\'GlennGreenwald@gmail.com\\', \\'protected\\': False, \\'link\\': {\\'text\\': \\'linktr.ee/glenngreenwald\\', \\'url\\': \\'https://linktr.ee/glenngreenwald\\', \\'tcourl\\': \\'https://t.co/rw7Uw16ZvA\\', \\'indices\\': (0, 23)}, \\'profileImageUrl\\': \\'https://pbs.twimg.com/profile_images/1092582027994509312/cpYWuYI9_normal.jpg\\', \\'profileBannerUrl\\': \\'https://pbs.twimg.com/profile_banners/16076032/1670619705\\', \\'label\\': None}, \\'replyCount\\': 1814, \\'retweetCount\\': 8513, \\'likeCount\\': 40283, \\'quoteCount\\': 585, \\'conversationId\\': 1625871270737809408, \\'lang\\': \\'en\\', \\'source\\': \\'<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>\\', \\'sourceUrl\\': \\'https://mobile.twitter.com\\', \\'sourceLabel\\': \\'Twitter Web App\\', \\'links\\': None, \\'media\\': None, \\'retweetedTweet\\': None, \\'quotedTweet\\': None, \\'inReplyToTweetId\\': None, \\'inReplyToUser\\': None, \\'mentionedUsers\\': None, \\'coordinates\\': None, \\'place\\': None, \\'hashtags\\': None, \\'cashtags\\': None, \\'card\\': None, \\'viewCount\\': 76140755, \\'vibe\\': None}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#links = tweets_df[tweets_df.columns[tweets_df.columns.isin(new_df.columns)==False]]['links'].value_counts().copy()\n",
    "\n",
    "tweets_df[tweets_df['rawContent']=='True']['quotedTweet'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "def clean_text(raw_text):    \n",
    "    cleaned_text = re.sub(r' \\'?(displayname|renderedDescription)\\'?: (.*?)(\\'|None),', '', raw_text)\n",
    "    cleaned_text = (cleaned_text\n",
    "                    .replace(\"'\",'\"')\n",
    "                    .replace('None','null')\n",
    "                    .replace('True','true')\n",
    "                    .replace('False','false'))\n",
    "    # cleaned_text = re.sub(r'(\\w+)\"(\\w+)', r\"\\1'\\2\", cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def deserialize(text):    \n",
    "    deserialized_texts = []\n",
    "    extract_dicts = re.findall(r'{.*?}',text)\n",
    "    \n",
    "    for str_dict in extract_dicts:\n",
    "        cleaned_text = clean_text(str_dict)\n",
    "\n",
    "        pattern = r'datetime.datetime\\(.*\\)'\n",
    "        cleaned_text = re.sub(f'({pattern})',r'\"\\1\"',cleaned_text)\n",
    "        \n",
    "        deserialized_text = json.loads(cleaned_text)\n",
    "        \n",
    "        if deserialized_text['created']!=None:\n",
    "            deserialized_text['created'] = eval(deserialized_text['created'])\n",
    "        \n",
    "        deserialized_texts.append(deserialized_text)\n",
    "\n",
    "    return deserialized_texts\n",
    "    \n",
    "    \n",
    "new_df = new_df.drop('inReplyToUser',axis=1)\n",
    "new_df['mentionedUsers'] = new_df['mentionedUsers'].apply(lambda x: deserialize(x) if type(x)==str else None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Model Bulding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, matutils, utils, models\n",
    "import spacy\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    url_pattern = [{\"label\": \"URL\",\n",
    "                    \"pattern\": [{\"LIKE_URL\": True}]}]\n",
    "\n",
    "    ruler = nlp.add_pipe('entity_ruler', before='ner')\n",
    "    ruler.add_patterns(url_pattern)\n",
    "    \n",
    "    texts_out = []\n",
    "    if type(texts)!=list:\n",
    "        texts = [texts]\n",
    "    \n",
    "    for text in texts:\n",
    "        # TODO: consider using nlp.pipe which should be faster\n",
    "        doc = nlp(text)\n",
    "        cleaned_text = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_ != 'URL' and not token.is_stop and token.pos_ in allowed_postags:\n",
    "                cleaned_text.append(token.lemma_)\n",
    "        final = ' '.join(cleaned_text)\n",
    "        texts_out.append(final)\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def create_ngrams(texts):\n",
    "    data_words = []\n",
    "    for text in texts:\n",
    "        new = utils.simple_preprocess(text)\n",
    "        data_words.append(new)\n",
    "\n",
    "    bigrams_phrases  = models.Phrases(data_words, min_count=3, threshold=50)\n",
    "    trigrams_phrases = models.Phrases(bigrams_phrases[data_words], threshold=50)\n",
    "\n",
    "    bigram  = models.phrases.Phraser(bigrams_phrases)\n",
    "    trigram = models.phrases.Phraser(trigrams_phrases)\n",
    "\n",
    "    data_bigrams = [bigram[doc] for doc in data_words]\n",
    "    data_bigrams_trigrams = [trigram[bigram[doc]] for doc in data_bigrams]\n",
    "    \n",
    "    return data_bigrams_trigrams\n",
    "\n",
    "\n",
    "def vectorize_texts(texts_ngrams):\n",
    "    id2word = corpora.Dictionary(texts_ngrams)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts_ngrams]\n",
    "    return id2word, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def make_custom_pipeline(steps):\n",
    "    for i, step in enumerate(steps):\n",
    "        steps.insert(i, (step[0], FunctionTransformer(step[1])))\n",
    "        steps.remove(step)\n",
    "\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "# with open('lemmatized_texts.txt', 'r', encoding=\"utf-8\") as f:\n",
    "#     lemmatized_texts = f.readlines()\n",
    "    \n",
    "# lemmatized_texts = [line.replace('\\n','') for line in lemmatized_texts]\n",
    "\n",
    "# lemmatized_texts = lemmatization(new_df['rawContent'])\n",
    "steps = [('lemmatization', lemmatization),\n",
    "         ('trigrams', create_ngrams),\n",
    "         ('vectorization', vectorize_texts)]\n",
    "\n",
    "preprocessing_pipeline = make_custom_pipeline(steps)\n",
    "id2word, corpus = preprocessing_pipeline.transform(new_df['rawContent'].values.tolist())\n",
    "\n",
    "# data_bigrams_trigrams = create_ngrams(lemmatized_texts)\n",
    "# id2word, corpus = vectorize_texts(data_bigrams_trigrams)\n",
    "\n",
    "#tfidf = models.TfidfModel(corpus, id2word=id2word)\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.6,\n",
    "#                                    min_df=5,\n",
    "#                                    ngram_range=(1,3))\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "# corpus = matutils.Sparse2Corpus(tfidf_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_value = 0.03\n",
    "# words = []\n",
    "# words_missing_in_tfidf = []\n",
    "\n",
    "# for i in range(0, len(corpus)):\n",
    "#     bow = corpus[i]\n",
    "#     low_value_words = []\n",
    "#     tfidf_ids = [id for id,_ in tfidf[bow]]\n",
    "#     bow_idf = [id for id,_ in bow]\n",
    "#     low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "#     drops = low_value_words+words_missing_in_tfidf\n",
    "    \n",
    "#     for item in drops:\n",
    "#         words.append(id2word[item])\n",
    "    \n",
    "#     # words with tfidf score of 0 will be missing\n",
    "#     words_missing_in_tfidf = [id for id in bow_idf if id not in tfidf_ids]\n",
    "#     new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "#     corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "def bayesian_tuning(model, params_grid: dict, texts, verbose=False):\n",
    "    # TODO: implement bayesian tuning\n",
    "    models_scores = {}\n",
    "    for i in range(95,170,5):\n",
    "        lda_model = model(corpus=params_grid['corpus'],\n",
    "                          num_topics=i,\n",
    "                          id2word=params_grid['id2word'],\n",
    "                          random_state=1,\n",
    "                          passes=10,\n",
    "                          per_word_topics=True)\n",
    "        \n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                             texts=texts, \n",
    "                                             corpus=params_grid['corpus'], \n",
    "                                             dictionary=params_grid['id2word']\n",
    "                                             )\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        \n",
    "        models_scores.update({lda_model: coherence_score})\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Topics {i:<3}: {coherence_score}')\n",
    "    \n",
    "    return models_scores\n",
    "\n",
    "\n",
    "params_grid = {'corpus':corpus,  \n",
    "               'num_topics':10, \n",
    "               'id2word':id2word, \n",
    "               'random_state':1, \n",
    "               'update_every':1, \n",
    "               'chunksize':3000, \n",
    "               'passes':2}\n",
    "\n",
    "# lda_models_scores = bayesian_tuning(LdaMulticore, params_grid, texts, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models import LdaMulticore\n",
    "import pyLDAvis\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         num_topics=20,\n",
    "                         id2word=id2word,\n",
    "                         random_state=1,\n",
    "                         passes=10,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_new_texts(texts: list[str]):\n",
    "    steps = [('lemmatization', lemmatization),\n",
    "            ('trigrams', create_ngrams)]\n",
    "\n",
    "    corpus_pipeline = make_custom_pipeline(steps)\n",
    "    texts_ngrams = corpus_pipeline.transform(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts_ngrams]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [id2word.doc2bow(text) for text in texts_ngrams]\n",
    "# lda_model.get_document_topics(corpus, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['vectorized']  = preprocessing_pipeline.transform(new_df['rawContent'].values.tolist())[1]\n",
    "new_df['TopicsProbs'] = new_df['vectorized'].apply(lambda x: dict(lda_model.get_document_topics(x, minimum_probability=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['TopicsProbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = pd.read_csv('Data/btc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df.iloc[0:1, 1:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesCount = new_df.groupby(new_df['date'].dt.date)['date'].count()\n",
    "new_df = new_df.merge(datesCount.rename('tweetsCount'), left_on=new_df['date'].dt.date, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_this = new_df.groupby(new_df['date'].dt.date).count()['date']\n",
    "plot_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval(text[13:-2])\n",
    "\n",
    "# #text.replace('(null)','-(1)-')\n",
    "# pattern = r'datetime.datetime\\(.*\\)'\n",
    "# text = re.sub(f'({pattern})',r'\"\\1\"',text)\n",
    "# deserialized_text = json.loads(text)\n",
    "# eval(re.findall(r'datetime.datetime\\(.*\\)', text)[0])\n",
    "# #deserialized_text\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "text = '{\"username\": \"_andyoneal\", \"id\": 19841555, \"displayname\": \"Andy O\"Neal\", \"rawDescription\": null, \"renderedDescription\": null, \"descriptionLinks\": null, \"verified\": null, \"created\": null, \"followersCount\": null, \"friendsCount\": null, \"statusesCount\": null, \"favouritesCount\": null, \"listedCount\": null, \"mediaCount\": null, \"location\": null, \"protected\": null, \"link\": null, \"profileImageUrl\": null, \"profileBannerUrl\": null, \"label\": null}'\n",
    "\n",
    "#text = re.sub(r'(\\w+)\"(\\w+)', r\"\\1'\\2\", text)\n",
    "#print(text)\n",
    "json.loads(json.dumps(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trash(text):\n",
    "    use_cols = ['username','id','verified','created','followersCount',\n",
    "                'friendsCount', 'statusesCount', 'favouritesCount',\n",
    "                'listedCount', 'mediaCount', 'location', 'protected', 'label']\n",
    "    \n",
    "    i=0\n",
    "    for word in text.split(\"'\"):\n",
    "        if word in use_cols:\n",
    "            print(i, word)\n",
    "            i+=1\n",
    "    return text\n",
    "\n",
    "text = \"[{'username': 'Jason', 'id': 3840, 'displayname': '@jason', 'rawDescription': None, 'renderedDescription': None, 'descriptionLinks': None, 'verified': None, 'created': None, 'followersCount': None, 'friendsCount': None, 'statusesCount': None, 'favouritesCount': None, 'listedCount': None, 'mediaCount': None, 'location': None, 'protected': None, 'link': None, 'profileImageUrl': None, 'profileBannerUrl': None, 'label': None}, {'username': 'DeanPreston', 'id': 59243738, 'displayname': 'Dean Preston', 'rawDescription': None, 'renderedDescription': None, 'descriptionLinks': None, 'verified': None, 'created': None, 'followersCount': None, 'friendsCount': None, 'statusesCount': None, 'favouritesCount': None, 'listedCount': None, 'mediaCount': None, 'location': None, 'protected': None, 'link': None, 'profileImageUrl': None, 'profileBannerUrl': None, 'label': None}, {'username': 'GrowSF', 'id': 1299405799517634560, 'displayname': 'GrowSF', 'rawDescription': None, 'renderedDescription': None, 'descriptionLinks': None, 'verified': None, 'created': None, 'followersCount': None, 'friendsCount': None, 'statusesCount': None, 'favouritesCount': None, 'listedCount': None, 'mediaCount': None, 'location': None, 'protected': None, 'link': None, 'profileImageUrl': None, 'profileBannerUrl': None, 'label': None}]\"\n",
    "# re.sub(r\"('displayname': )'(.*)' rawDescription\", r'--VOID--', text)\n",
    "\n",
    "# drop_trash(text)\n",
    "#re.split(r\"'(.*)':?\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total_anus = new_df.iloc[-2,14]\n",
    "\n",
    "#re.findall(r'({.*})',total_anus)\n",
    "json.loads(json.dumps(total_anus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
